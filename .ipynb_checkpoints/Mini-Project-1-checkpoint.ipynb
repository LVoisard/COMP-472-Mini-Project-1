{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1, team Kool Kids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "labels = 'Post', 'Emotion', 'Sentiment'\n",
    "file = gzip.open('goemotions.json.gz', 'rb')\n",
    "entries = json.load(file)\n",
    "dataset = pd.DataFrame(entries, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extract posts and labels in a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55c1f28e679f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0memotions_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentiments_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "posts = dataset[labels[0]]\n",
    "emotions = dataset[labels[1]]\n",
    "sentiments = dataset[labels[2]]\n",
    "emotions_distribution = dataset.pivot_table(columns=labels[1], aggfunc='size')\n",
    "sentiments_distribution = dataset.pivot_table(columns=labels[2], aggfunc='size')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plotting the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (emo, sent) = plt.subplots(1, 2)\n",
    "\n",
    "# Emotions Pie Chart\n",
    "emo.set_title(\"Emotions\")\n",
    "emo.pie(emotions_distribution.values, labels=emotions_distribution.keys(),\n",
    "            shadow=False, startangle=90, rotatelabels=True)\n",
    "\n",
    "# Sentiment Pie Chart\n",
    "sent.set_title(\"Sentiment\")\n",
    "sent.pie(\n",
    "    sentiments_distribution.values, labels=sentiments_distribution.keys(), \n",
    "    autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Words as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Displaying dataset tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "post_vectorizer = CountVectorizer()\n",
    "X = post_vectorizer.fit_transform(dataset[labels[0]])\n",
    "print('There are', X.shape[1], 'unique tokens')\n",
    "\n",
    "emotions_label_encoder = LabelEncoder()\n",
    "Y = emotions_label_encoder.fit_transform(emotions)\n",
    "\n",
    "sentiments_label_encoder = LabelEncoder()\n",
    "Z = sentiments_label_encoder.fit_transform(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec9536c86ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m post_train, post_test, emotion_train, emotion_test, sentiment_train, sentiment_test = test_split.train_test_split(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection.tests import test_split\n",
    "\n",
    "post_train, post_test, emotion_train, emotion_test, sentiment_train, sentiment_test = test_split.train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    Z,\n",
    "    test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classifier Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Base-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_mnb_emotion = MultinomialNB()\n",
    "base_mnb_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "base_mnb_sentiment = MultinomialNB()\n",
    "base_mnb_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Base-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dt_emotion = DecisionTreeClassifier()\n",
    "base_dt_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "base_dt_sentiment = DecisionTreeClassifier()\n",
    "base_dt_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Base-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mlp_emotion = MLPClassifier(max_iter=1)\n",
    "print(base_mlp_emotion.fit(post_train, emotion_train))\n",
    "\n",
    "\n",
    "base_mlp_sentiment = MLPClassifier(max_iter=1)\n",
    "print(base_mlp_sentiment.fit(post_train, sentiment_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Top-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0, 0.25, 0.5, 0.75]}\n",
    "top_mnb_emotion = GridSearchCV(MultinomialNB(), param_grid)\n",
    "top_mnb_emotion.fit(post_test, emotion_test)\n",
    "\n",
    "top_mnb_sentiment = GridSearchCV(MultinomialNB(), param_grid)\n",
    "top_mnb_sentiment.fit(post_test, sentiment_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Top-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'criterion': ['entropy'],\n",
    "        'max_depth': [2, 8],\n",
    "        'min_samples_split': [2, 4, 6]\n",
    "    }\n",
    "top_dt_emotion = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "top_dt_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "top_dt_sentiment = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "top_dt_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Top-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "    'hidden_layer_sizes': [(30,50),(10,10,10)],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "top_mlp_emotion = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "top_mlp_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "\n",
    "top_mlp_sentiment = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "top_mlp_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Base MNB')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_mnb_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_mnb_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_mnb_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_mnb_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Base DT')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_dt_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_dt_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_dt_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_dt_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Base MLP')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_mlp_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_mlp_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_mlp_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_mlp_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top MNB')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_mnb_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_mnb_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_mnb_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_mnb_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top DT')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_dt_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_dt_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_dt_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_dt_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top MLP')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_mlp_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_mlp_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment',)\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_mlp_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_mlp_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Exploration (Removing Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_vectorizer = CountVectorizer(stop_words='english')\n",
    "X = post_vectorizer.fit_transform(dataset[labels[0]])\n",
    "print('There are', X.shape[1], 'unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Base-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mnb_emotion = MultinomialNB()\n",
    "base_mnb_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "base_mnb_sentiment = MultinomialNB()\n",
    "base_mnb_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Base-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dt_emotion = DecisionTreeClassifier()\n",
    "base_dt_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "base_dt_sentiment = DecisionTreeClassifier()\n",
    "base_dt_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 Base-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mlp_emotion = MLPClassifier(max_iter=1)\n",
    "print(base_mlp_emotion.fit(post_train, emotion_train))\n",
    "\n",
    "\n",
    "base_mlp_sentiment = MLPClassifier(max_iter=1)\n",
    "print(base_mlp_sentiment.fit(post_train, sentiment_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.4 Top-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0, 0.25, 0.5, 0.75]}\n",
    "top_mnb_emotion = GridSearchCV(MultinomialNB(), param_grid)\n",
    "top_mnb_emotion.fit(post_test, emotion_test)\n",
    "\n",
    "top_mnb_sentiment = GridSearchCV(MultinomialNB(), param_grid)\n",
    "top_mnb_sentiment.fit(post_test, sentiment_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.5 Top-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'criterion': ['entropy'],\n",
    "        'max_depth': [2, 8],\n",
    "        'min_samples_split': [2, 4, 6]\n",
    "    }\n",
    "top_dt_emotion = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "top_dt_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "top_dt_sentiment = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "top_dt_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.6 Top-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "    'hidden_layer_sizes': [(30,50),(10,10,10)],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "top_mlp_emotion = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "top_mlp_emotion.fit(post_train, emotion_train)\n",
    "\n",
    "\n",
    "top_mlp_sentiment = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "top_mlp_sentiment.fit(post_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print('Base MNB')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_mnb_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_mnb_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_mnb_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_mnb_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Base DT')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_dt_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_dt_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_dt_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_dt_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Base MLP')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, base_mlp_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, base_mlp_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, base_mlp_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, base_mlp_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top MNB')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_mnb_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_mnb_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_mnb_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_mnb_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top DT')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_dt_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_dt_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_dt_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_dt_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top MLP')\n",
    "print('Emotion')\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(emotion_test, top_mlp_emotion.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, top_mlp_emotion.predict(post_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment',)\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(sentiment_test, top_mlp_sentiment.predict(post_test)))\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, top_mlp_sentiment.predict(post_test), target_names=sentiments_label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Embedding as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "\n",
    "\n",
    "model = load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "postsVec = dataset[labels[0]].apply(word_tokenize)\n",
    "posts_vec_train, posts_vec_test = test_split.train_test_split(postsVec, test_size=0.20)\n",
    "\n",
    "words_train = 0\n",
    "\n",
    "for i, post in enumerate(posts_vec_train):\n",
    "    for word in post:\n",
    "        words_train = words_train + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(posts_vec_train), 'sentences')\n",
    "print('There are', words_train, 'tokens')\n",
    "print('Training only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Post Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_embedding_train = np.zeros((len(posts_vec_train), model.vector_size))\n",
    "for i, post in enumerate(posts_vec_train):\n",
    "    post_vec = np.zeros((model.vector_size,))\n",
    "    words = 0\n",
    "    for word in post:\n",
    "        if word in model:\n",
    "            words = words + 1\n",
    "            post_vec = np.add(post_vec, model[word])            \n",
    "    if words == 0:\n",
    "        words = 1\n",
    "    post_embedding_train[i] = np.divide(post_vec, words)\n",
    "    \n",
    "post_embedding_test = np.zeros((len(posts_vec_test), model.vector_size))\n",
    "for i, post in enumerate(posts_vec_test):\n",
    "    post_vec = np.zeros((model.vector_size,))\n",
    "    words = 0\n",
    "    for word in post:\n",
    "        if word in model:\n",
    "            words = words + 1\n",
    "            post_vec = np.add(post_vec, model[word])\n",
    "    if words == 0:\n",
    "        words = 1    \n",
    "    post_embedding_test[i] = np.divide(post_vec, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = 0\n",
    "total_words = 0\n",
    "for i, post in enumerate(posts_vec_train):\n",
    "    for word in post:\n",
    "        total_words = total_words + 1\n",
    "        if word in model:\n",
    "            hits = hits + 1\n",
    "print('Training hit rate', hits / total_words)\n",
    "\n",
    "hits = 0\n",
    "total_words = 0\n",
    "for i, post in enumerate(posts_vec_test):\n",
    "    for word in post:\n",
    "        total_words = total_words + 1\n",
    "        if word in model:\n",
    "            hits = hits + 1\n",
    "print('Testing hit rate', hits / total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Base-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_base_mlp_emotion = MLPClassifier()\n",
    "w2v_base_mlp_emotion.fit(post_embedding_train, emotion_train)\n",
    "\n",
    "w2v_base_mlp_sentiment = MLPClassifier()\n",
    "w2v_base_mlp_sentiment.fit(post_embedding_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Top-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "        'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "        'solver': ['adam', 'sgd']\n",
    "    }\n",
    "w2v_top_mlp_emotion = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "w2v_top_mlp_emotion.fit(post_embedding_train, emotion_train)\n",
    "\n",
    "w2v_top_mlp_sentiment = GridSearchCV(MLPClassifier(max_iter=1), param_grid)\n",
    "w2v_top_mlp_sentiment.fit(post_embedding_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.7 Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Base MLP')\n",
    "print('Emotion')\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, w2v_base_mlp_emotion.predict(post_embedding_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment')\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, w2v_base_mlp_sentiment.predict(post_embedding_test), target_names=sentiments_label_encoder.classes_))\n",
    "\n",
    "print('Top MLP')\n",
    "print('Emotion')\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(emotion_test, w2v_top_mlp_emotion.predict(post_embedding_test), target_names=emotions_label_encoder.classes_))\n",
    "\n",
    "print('Sentiment',)\n",
    "print('\\n Classification Report \\n')\n",
    "print(classification_report(sentiment_test, w2v_top_mlp_sentiment.predict(post_embedding_test), target_names=sentiments_label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
